# Milestone 3: Hyperparameter Optimization
In Milestone 3, Team 4 (Patricia Dzwill and Elizabeth Finnegan) were assigned to improve baseline performance with the Hyperparameter Optimization (HPO) method SMAC. HPO’s methods are designed to improve a model by determining which hyperparameters (parameters used to control the learning process) are the most useful. Tuning the hyperparameters can be done in several ways, but Team’s 4 method is Sequential Model-Based Algorithm Configuration.  SMAC implements a Bayesian Optimization approach and was designed as a python package by AutoML Groups of the Universities of Hannover and Freiburg (as requested by the Groups, here is their JMLR paper: https://jmlr.org/papers/v23/21-0888.html). SMAC3, the python package on Github, is able to improve the performance of a model within a few evaluations because of an aggressive racing mechanism implemented within it. Bayesian optimization cares only for input and output, and does not assume any functional forms (this is called a black box method). SMAC can be used with continuous, categorical, hierarchical and conditional hyperparameters (for our tasks, we used a UniformFloatParameter). We set the objective of our SMAC method to optimize accuracy of the model, but we could have also chosen to optimize run-time or cross-validation. There are various other parts of SMAC’s complex glossary, which can be found within the documentation provided by our distinguished colleagues at AutoML.



Milestone 3 proved to be the most challenging milestone yet, in Team 4’s humble experiences. SMAC, while extremely useful, was a tad difficult to comprehend in Getting Started with it. It requires four components: a configuration space, a scenario, a target function, and a facade. However, Team 4’s resilience led to them choosing decent components in the end. The configuration space had the sole hyperparameter of batch size. Unfortunately, Team 4 could not have any others because of time restraints and Colab's restrictions. For instance, it would crash as we increased time for SMAC or it would run for hours. As previously mentioned, our scenario had SMAC optimized for accuracy. Additionally, it had restrictions placed on the number of runs, the time it could run, and the setting of the configuration space. Our facade relied on the SMAC4HPO function (as it was the Hyperparameter Optimization facade that we needed to use). Finally, our target function was our previous model. Team 4 made these decisions based on extensive research in the SMAC documentation, class notes, and general sources on Hyperparameter Optimization. Additionally, Team 4 wants to make the note that if one is sharing a Google Colab notebook, no two people should work on the same one at the same time and try to access the ngrok site with the same authentication token (this will not work as an account is limited to 1 ngrok site open at once, and will only cause headaches). Our final comment is that we did try to use a random forest classifier but we got lost in the forest. Beware using things that do not have maps. 



Our results were the following ten images from the validation set, a graph showing the improved training and validation loss vs. epochs, and a graph with the precision-recall curve. Comparing results to our milestone 2 results (which can be seen in baseline-performance.MD), Team 4 finds that our optimization method has improved our model. All of our results support this statement, as the losses are lower and the precision is higher. As such, Team 4 celebrates the end of Milestone 3 as a success for all “Introduction to Data Science” students. 

![output0(1)](https://user-images.githubusercontent.com/117039859/202954236-d4e7cbf1-8f34-4110-acf0-d0d9b95b234f.png)
![output1(1)](https://user-images.githubusercontent.com/117039859/202954241-a19bd258-dbd4-410c-a4bc-0c0d1fb556e5.png)
![output2(1)](https://user-images.githubusercontent.com/117039859/202954249-4603af30-21d2-4f76-892d-05ee10d3c0ab.png)
![output3(1)](https://user-images.githubusercontent.com/117039859/202954259-21b9e96c-96f2-49e4-ba35-02b241fd09da.png)
![output4(1)](https://user-images.githubusercontent.com/117039859/202954267-b35f8331-917c-4fcf-a490-2269d756d153.png)
![output5(1)](https://user-images.githubusercontent.com/117039859/202954274-d4af9615-c12b-43df-9a5b-0cf9b4cdc442.png)
![output6(1)](https://user-images.githubusercontent.com/117039859/202954286-34df6029-5a14-40dd-812b-77591c0782c5.png)
![output7(1)](https://user-images.githubusercontent.com/117039859/202954292-dbab1ea4-e832-45f1-8e42-cf7d1bdeda3f.png)
![output8(1)](https://user-images.githubusercontent.com/117039859/202954298-80134b9b-3bbc-4b7d-a84a-7c66c69e42df.png)
![output9(1)](https://user-images.githubusercontent.com/117039859/202954300-93a28601-37ad-4200-a163-1b0bed4be92b.png)
![LossvEpochs(1)](https://user-images.githubusercontent.com/117039859/202954307-fac2ec6b-a624-4133-b74c-057921e860fb.png)
![PrecisionvsRecall(1)](https://user-images.githubusercontent.com/117039859/202954311-c25b7fd0-45ec-4118-8b4b-1fc7670a91e8.png)

